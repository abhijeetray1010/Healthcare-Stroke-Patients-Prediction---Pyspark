{"cells":[{"cell_type":"markdown","source":["##Healthcare Stroke Patients Prediction - Pyspark\n\nTeam members - Abhijeet Ray, Deepak Rao , AKhil Menon, Kushagra Sen"],"metadata":{}},{"cell_type":"markdown","source":["## Objective \n* To forecast whether a patient can have stroke or not using the given information of patients. It is a classification problem, where we will try to predict the probability of an observation belonging to a category (in our case probability of having a stroke)\n\n* Here we have clinical measurements (e.g. Hypertension, heart_disease, age, smoking_status) for a number of patients, as well as information about whether each patient has had a stroke. In practice, we are developing our model to accurately predict stroke risk for future patients based on their clinical measurements."],"metadata":{}},{"cell_type":"markdown","source":["### Understand stroke\n#### What is a stroke\n\n* A stroke is a “brain attack”. It can happen to anyone at any time. It occurs when blood flow to an area of brain is cut off. When this happens, brain cells are deprived of oxygen and begin to die. When brain cells die during a stroke, abilities controlled by that area of the brain such as memory and muscle control are lost.\n\n#### Stroke are the world’s biggest killers\n\n* 800,000 strokes per year in the US\n* Fitfh leading cause of death in the US\n* Leading cause of adult disability in the US\n* 80% are preventable"],"metadata":{}},{"cell_type":"markdown","source":["#### Understanding stroke attributes in the dataset\n\n* This dataset contains clinical measurements of 43401 patients. Description of this dataset can be viewed on the Kaggle website, where the data was obtained (https://www.kaggle.com/asaumya/healthcare-dataset-stroke-data).\n\n##### Data Description\n* ID: Patient's ID, probably irrelevant unless to avoid duplicates\n\n* GENDER - SEX (male ; female)\n\n* AGE - age in years\n\n* HYPERTENSION: (0- No ; 1- Yes) Hypertension is another name for high blood pressure, Can lead to severe complications and increases the risk of heart disease, stroke, and death. Normal blood pressure is 120 over 80 mm of mercury (mmHg), but hypertension is higher than 130 over 80 mmHg. (Source: https://www.medicalnewstoday.com/articles/150109.php)\n\n* HEART_DISEASE: (0- No ; 1- Yes), could cause increase stroke risks.\n     \n* EVER_MARRIED: (YES or NO) Lifestyle factor. Is this relevant? -We will need to test.\n\n* TYPE_OF_WORK: Did the person worked as a Government servant or in a Private organization or was Self employed? Lifestyle factor. Is this relevant? -We will need to test.\n\n* RESIDENCE: Home location (Rural or Urban), Lifestyle factor. Is this relevant? -We will need to test.\n\n* AVG_GLUCOSE: Average Glucose level in the person, could be relevant.\n\n* BMI: Body Mass Index. Could be relevant, to test.\n\n     *An index for assessing overweight and underweight, obtained by dividing body weight in kg by height in m^2\n     \n     *A measure of 25 or more is considered overweight.\n     \n* Smoking_Status : Smoking habbits (Smoking, Formerly smoked, occassional smoker), its a Lifestyle factor.\n\n##### Predictor Field\n* Stroke: Did the person had stroke ? (0-No ; 1-Yes)"],"metadata":{}},{"cell_type":"markdown","source":["### Importing necessary modules"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nimport pyspark.sql as sparksql\nimport pandas as pd\nimport numpy as np\n#Data visualisation libraries \nimport matplotlib.pyplot as plt   \nimport seaborn as sns\n#importing ml features\nfrom pyspark.ml.feature import StringIndexer,VectorAssembler\nfrom pyspark.ml.classification import LogisticRegression\nfrom pyspark.ml import Pipeline,Model\nfrom pyspark.ml.feature import StringIndexer"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["%sh\nwget -O train https://www.dropbox.com/s/p3sy3sxj71gxjtx/train_2v.csv?dl=0"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2020-01-18 18:19:35--  https://www.dropbox.com/s/p3sy3sxj71gxjtx/train_2v.csv?dl=0\nResolving www.dropbox.com (www.dropbox.com)... 162.125.1.1, 2620:100:6016:1::a27d:101\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.1.1|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /s/raw/p3sy3sxj71gxjtx/train_2v.csv [following]\n--2020-01-18 18:19:36--  https://www.dropbox.com/s/raw/p3sy3sxj71gxjtx/train_2v.csv\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc8eef496296900a67cb97f7aa1e.dl.dropboxusercontent.com/cd/0/inline/AwZyLSfiS54CYN-qU6OEUXuprABgsutlKkQkNrYy_qus9-eIVjHcn5tYz361Kz28IMda4vU9EqdTNcxRkONVJoNAqi2VgZLEi4b8wg-5ULot3DiYRz86ruQ8j88MOeEST08/file# [following]\n--2020-01-18 18:19:36--  https://uc8eef496296900a67cb97f7aa1e.dl.dropboxusercontent.com/cd/0/inline/AwZyLSfiS54CYN-qU6OEUXuprABgsutlKkQkNrYy_qus9-eIVjHcn5tYz361Kz28IMda4vU9EqdTNcxRkONVJoNAqi2VgZLEi4b8wg-5ULot3DiYRz86ruQ8j88MOeEST08/file\nResolving uc8eef496296900a67cb97f7aa1e.dl.dropboxusercontent.com (uc8eef496296900a67cb97f7aa1e.dl.dropboxusercontent.com)... 162.125.1.6, 2620:100:6016:6::a27d:106\nConnecting to uc8eef496296900a67cb97f7aa1e.dl.dropboxusercontent.com (uc8eef496296900a67cb97f7aa1e.dl.dropboxusercontent.com)|162.125.1.6|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2635787 (2.5M) [text/plain]\nSaving to: ‘train’\n\n     0K .......... .......... .......... .......... ..........  1% 3.16M 1s\n    50K .......... .......... .......... .......... ..........  3% 10.9M 0s\n   100K .......... .......... .......... .......... ..........  5% 10.7M 0s\n   150K .......... .......... .......... .......... ..........  7% 15.1M 0s\n   200K .......... .......... .......... .......... ..........  9% 18.9M 0s\n   250K .......... .......... .......... .......... .......... 11% 36.8M 0s\n   300K .......... .......... .......... .......... .......... 13% 25.5M 0s\n   350K .......... .......... .......... .......... .......... 15% 20.6M 0s\n   400K .......... .......... .......... .......... .......... 17% 45.0M 0s\n   450K .......... .......... .......... .......... .......... 19% 30.0M 0s\n   500K .......... .......... .......... .......... .......... 21% 34.6M 0s\n   550K .......... .......... .......... .......... .......... 23% 99.1M 0s\n   600K .......... .......... .......... .......... .......... 25% 55.9M 0s\n   650K .......... .......... .......... .......... .......... 27% 62.7M 0s\n   700K .......... .......... .......... .......... .......... 29% 65.3M 0s\n   750K .......... .......... .......... .......... .......... 31% 58.4M 0s\n   800K .......... .......... .......... .......... .......... 33% 69.4M 0s\n   850K .......... .......... .......... .......... .......... 34% 42.9M 0s\n   900K .......... .......... .......... .......... .......... 36% 63.4M 0s\n   950K .......... .......... .......... .......... .......... 38%  117M 0s\n  1000K .......... .......... .......... .......... .......... 40% 96.8M 0s\n  1050K .......... .......... .......... .......... .......... 42% 70.0M 0s\n  1100K .......... .......... .......... .......... .......... 44%  140M 0s\n  1150K .......... .......... .......... .......... .......... 46% 62.6M 0s\n  1200K .......... .......... .......... .......... .......... 48%  114M 0s\n  1250K .......... .......... .......... .......... .......... 50%  139M 0s\n  1300K .......... .......... .......... .......... .......... 52%  120M 0s\n  1350K .......... .......... .......... .......... .......... 54%  129M 0s\n  1400K .......... .......... .......... .......... .......... 56%  140M 0s\n  1450K .......... .......... .......... .......... .......... 58%  132M 0s\n  1500K .......... .......... .......... .......... .......... 60%  126M 0s\n  1550K .......... .......... .......... .......... .......... 62%  112M 0s\n  1600K .......... .......... .......... .......... .......... 64%  138M 0s\n  1650K .......... .......... .......... .......... .......... 66%  123M 0s\n  1700K .......... .......... .......... .......... .......... 67%  132M 0s\n  1750K .......... .......... .......... .......... .......... 69%  116M 0s\n  1800K .......... .......... .......... .......... .......... 71%  121M 0s\n  1850K .......... .......... .......... .......... .......... 73%  126M 0s\n  1900K .......... .......... .......... .......... .......... 75% 8.70M 0s\n  1950K .......... .......... .......... .......... .......... 77%  100M 0s\n  2000K .......... .......... .......... .......... .......... 79%  115M 0s\n  2050K .......... .......... .......... .......... .......... 81%  231M 0s\n  2100K .......... .......... .......... .......... .......... 83%  222M 0s\n  2150K .......... .......... .......... .......... .......... 85% 22.7M 0s\n  2200K .......... .......... .......... .......... .......... 87%  116M 0s\n  2250K .......... .......... .......... .......... .......... 89%  220M 0s\n  2300K .......... .......... .......... .......... .......... 91%  213M 0s\n  2350K .......... .......... .......... .......... .......... 93%  187M 0s\n  2400K .......... .......... .......... .......... .......... 95% 27.1M 0s\n  2450K .......... .......... .......... .......... .......... 97%  106M 0s\n  2500K .......... .......... .......... .......... .......... 99%  135M 0s\n  2550K .......... .......... ....                            100%  161M=0.07s\n\n2020-01-18 18:19:36 (37.2 MB/s) - ‘train’ saved [2635787/2635787]\n\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["### Loading the dataset"],"metadata":{}},{"cell_type":"code","source":["train = spark.read.csv(path='file:///databricks/driver/train', inferSchema=True,header=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["### Exploring the data"],"metadata":{}},{"cell_type":"code","source":["train.dtypes"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[4]: [(&#39;id&#39;, &#39;int&#39;),\n (&#39;gender&#39;, &#39;string&#39;),\n (&#39;age&#39;, &#39;double&#39;),\n (&#39;hypertension&#39;, &#39;int&#39;),\n (&#39;heart_disease&#39;, &#39;int&#39;),\n (&#39;ever_married&#39;, &#39;string&#39;),\n (&#39;work_type&#39;, &#39;string&#39;),\n (&#39;Residence_type&#39;, &#39;string&#39;),\n (&#39;avg_glucose_level&#39;, &#39;double&#39;),\n (&#39;bmi&#39;, &#39;double&#39;),\n (&#39;smoking_status&#39;, &#39;string&#39;),\n (&#39;stroke&#39;, &#39;int&#39;)]</div>"]}}],"execution_count":11},{"cell_type":"code","source":["train.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[5]: [&#39;id&#39;,\n &#39;gender&#39;,\n &#39;age&#39;,\n &#39;hypertension&#39;,\n &#39;heart_disease&#39;,\n &#39;ever_married&#39;,\n &#39;work_type&#39;,\n &#39;Residence_type&#39;,\n &#39;avg_glucose_level&#39;,\n &#39;bmi&#39;,\n &#39;smoking_status&#39;,\n &#39;stroke&#39;]</div>"]}}],"execution_count":12},{"cell_type":"code","source":["train.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: integer (nullable = true)\n-- gender: string (nullable = true)\n-- age: double (nullable = true)\n-- hypertension: integer (nullable = true)\n-- heart_disease: integer (nullable = true)\n-- ever_married: string (nullable = true)\n-- work_type: string (nullable = true)\n-- Residence_type: string (nullable = true)\n-- avg_glucose_level: double (nullable = true)\n-- bmi: double (nullable = true)\n-- smoking_status: string (nullable = true)\n-- stroke: integer (nullable = true)\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["train.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n   id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level| bmi| smoking_status|stroke|\n+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\n30669|  Male| 3.0|           0|            0|          No|     children|         Rural|            95.12|18.0|           null|     0|\n30468|  Male|58.0|           1|            0|         Yes|      Private|         Urban|            87.96|39.2|   never smoked|     0|\n16523|Female| 8.0|           0|            0|          No|      Private|         Urban|           110.89|17.6|           null|     0|\n56543|Female|70.0|           0|            0|         Yes|      Private|         Rural|            69.04|35.9|formerly smoked|     0|\n46136|  Male|14.0|           0|            0|          No| Never_worked|         Rural|           161.28|19.1|           null|     0|\n32257|Female|47.0|           0|            0|         Yes|      Private|         Urban|           210.95|50.1|           null|     0|\n52800|Female|52.0|           0|            0|         Yes|      Private|         Urban|            77.59|17.7|formerly smoked|     0|\n41413|Female|75.0|           0|            1|         Yes|Self-employed|         Rural|           243.53|27.0|   never smoked|     0|\n15266|Female|32.0|           0|            0|         Yes|      Private|         Rural|            77.67|32.3|         smokes|     0|\n28674|Female|74.0|           1|            0|         Yes|Self-employed|         Urban|           205.84|54.6|   never smoked|     0|\n10460|Female|79.0|           0|            0|         Yes|     Govt_job|         Urban|            77.08|35.0|           null|     0|\n64908|  Male|79.0|           0|            1|         Yes|      Private|         Urban|            57.08|22.0|formerly smoked|     0|\n63884|Female|37.0|           0|            0|         Yes|      Private|         Rural|           162.96|39.4|   never smoked|     0|\n37893|Female|37.0|           0|            0|         Yes|      Private|         Rural|             73.5|26.1|formerly smoked|     0|\n67855|Female|40.0|           0|            0|         Yes|      Private|         Rural|            95.04|42.4|   never smoked|     0|\n25774|  Male|35.0|           0|            0|          No|      Private|         Rural|            85.37|33.0|   never smoked|     0|\n19584|Female|20.0|           0|            0|          No|      Private|         Urban|            84.62|19.7|         smokes|     0|\n24447|Female|42.0|           0|            0|         Yes|      Private|         Rural|            82.67|22.5|   never smoked|     0|\n49589|Female|44.0|           0|            0|         Yes|     Govt_job|         Urban|            57.33|24.6|         smokes|     0|\n17986|Female|79.0|           0|            1|         Yes|Self-employed|         Urban|            67.84|25.2|         smokes|     0|\n+-----+------+----+------------+-------------+------------+-------------+--------------+-----------------+----+---------------+------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["### Visualizing the Data"],"metadata":{}},{"cell_type":"code","source":["pdata = train.toPandas()\ng = sns.pairplot(pdata, size=1.2)\ndisplay(g.fig)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["fig, ax = plt.subplots()\nfig.set_size_inches(7, 5)\nsns.heatmap(pdata.corr(),annot=True ,cmap='magma').set_title('Correlation Factors Heat Map', color='black', size='20')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["* It is clearly evident that Age and heart disease are the factors that are highly correlated with Stroke, followed by avg_glucose_level, hypertension and then bmi"],"metadata":{}},{"cell_type":"markdown","source":["#### Training feature analysis"],"metadata":{}},{"cell_type":"code","source":["# create DataFrame as a temporary view for SQL queries\ntrain.createOrReplaceTempView('table')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":20},{"cell_type":"markdown","source":["#### Work type vs stroke (influence of work type on getting stroke)"],"metadata":{}},{"cell_type":"code","source":["# sql query to find the number of people in specific work_type who have had stroke and not\nspark.sql(\"SELECT work_type, COUNT(work_type) as work_type_count \\\n          FROM table WHERE stroke == 1 \\\n          GROUP BY work_type \\\n          ORDER BY COUNT(work_type) DESC\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+---------------+\n    work_type|work_type_count|\n+-------------+---------------+\n      Private|            441|\nSelf-employed|            251|\n     Govt_job|             89|\n     children|              2|\n+-------------+---------------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"markdown","source":["* The most affected work_type persons are private followed by self-employed."],"metadata":{}},{"cell_type":"markdown","source":["#### Gender vs Stroke (is strokes related to gender ? )"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT gender, COUNT(gender) as gender_count, COUNT(gender)*100/(SELECT COUNT(gender) FROM table WHERE gender == 'Male') as    percentage \\\n          FROM table WHERE stroke== 1 AND gender = 'Male' \\\n          GROUP BY gender\").show()\nspark.sql(\"SELECT gender, COUNT(gender) as gender_count, COUNT(gender)*100/(SELECT COUNT(gender) FROM table WHERE gender == 'Female') as percentage \\\n          FROM table WHERE stroke== 1 AND gender = 'Female' \\\n          GROUP BY gender\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------+------------+------------------+\ngender|gender_count|        percentage|\n+------+------------+------------------+\n  Male|         352|1.9860076732114647|\n+------+------------+------------------+\n\n+------+------------+------------------+\ngender|gender_count|        percentage|\n+------+------------+------------------+\nFemale|         431|1.6793298266121177|\n+------+------------+------------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"markdown","source":["* 1.68% male and almost 2% male had stroke."],"metadata":{}},{"cell_type":"markdown","source":["#### Age vs Stroke (Now we will see influence of age on stroke)"],"metadata":{}},{"cell_type":"code","source":["spark.sql(\"SELECT COUNT(age)*100/(SELECT COUNT(age) FROM table WHERE stroke ==1) as percentage \\\n          FROM table \\\n          WHERE stroke == 1 AND age<50\").show()\nspark.sql(\"SELECT COUNT(age)*100/(SELECT COUNT(age) FROM table WHERE stroke ==1) as percentage \\\n          FROM table \\\n          WHERE stroke == 1 AND age>=50\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\n      percentage|\n+----------------+\n8.42911877394636|\n+----------------+\n\n+-----------------+\n       percentage|\n+-----------------+\n91.57088122605364|\n+-----------------+\n\n</div>"]}}],"execution_count":28},{"cell_type":"markdown","source":["* It is clearly evident that older age have a strong correlation with Strokes, as 91.5% stroke had occured for people who are more than 50 years old and only 8% stroke for people whose age is less than 50"],"metadata":{}},{"cell_type":"markdown","source":["### Cleaning up training data"],"metadata":{}},{"cell_type":"code","source":["train.describe().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+------------------+------------------+---------------+-------------------+\nsummary|               id|gender|               age|       hypertension|      heart_disease|ever_married|work_type|Residence_type| avg_glucose_level|               bmi| smoking_status|             stroke|\n+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+------------------+------------------+---------------+-------------------+\n  count|            43400| 43400|             43400|              43400|              43400|       43400|    43400|         43400|             43400|             41938|          30108|              43400|\n   mean|36326.14235023042|  null| 42.21789400921646|0.09357142857142857|0.04751152073732719|        null|     null|          null|104.48274999999916|28.605038390004545|           null|0.01804147465437788|\n stddev|21072.13487918279|  null|22.519648680503554|  0.291234906309397|0.21273274050209726|        null|     null|          null|  43.1117509512961| 7.770020497238766|           null|0.13310292280179215|\n    min|                1|Female|              0.08|                  0|                  0|          No| Govt_job|         Rural|              55.0|              10.1|formerly smoked|                  0|\n    max|            72943| Other|              82.0|                  1|                  1|         Yes| children|         Urban|            291.05|              97.6|         smokes|                  1|\n+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+------------------+------------------+---------------+-------------------+\n\n</div>"]}}],"execution_count":31},{"cell_type":"markdown","source":["d\n * Here we see that the count for each of the columns is 43400 except \"smoking status\" and \"bmi\". This indicates there are few missing values in \"smoking_status\" and \"bmi\"\n * Also there are few categorical data (gender, ever_married, work_type, Residence_type, smoking_status) which we need to covert using one hot encoding"],"metadata":{}},{"cell_type":"code","source":["# fill in missing values for column \"smoking status\"\n# As \"smoking status\" column is categorical data, we will add one data type \"No Info\" for the missing one\n\ntrain_f = train.na.fill('No Info', subset=['smoking_status'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"code","source":["# fill in missing values for column \"bmi\" \n# as column \"bmi\" is numerical data , we will fill the missing values with mean\n\nfrom pyspark.sql.functions import mean\nmean = train_f.select(mean(train_f['bmi'])).collect()\nmean_bmi = mean[0][0]\ntrain_f = train_f.na.fill(mean_bmi,['bmi'])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":34},{"cell_type":"code","source":["train_f.describe().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+------------------+------------------+--------------+-------------------+\nsummary|               id|gender|               age|       hypertension|      heart_disease|ever_married|work_type|Residence_type| avg_glucose_level|               bmi|smoking_status|             stroke|\n+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+------------------+------------------+--------------+-------------------+\n  count|            43400| 43400|             43400|              43400|              43400|       43400|    43400|         43400|             43400|             43400|         43400|              43400|\n   mean|36326.14235023042|  null| 42.21789400921646|0.09357142857142857|0.04751152073732719|        null|     null|          null|104.48274999999916|28.605038390005145|          null|0.01804147465437788|\n stddev|21072.13487918279|  null|22.519648680503554|  0.291234906309397|0.21273274050209726|        null|     null|          null|  43.1117509512961| 7.638023372051845|          null|0.13310292280179215|\n    min|                1|Female|              0.08|                  0|                  0|          No| Govt_job|         Rural|              55.0|              10.1|       No Info|                  0|\n    max|            72943| Other|              82.0|                  1|                  1|         Yes| children|         Urban|            291.05|              97.6|        smokes|                  1|\n+-------+-----------------+------+------------------+-------------------+-------------------+------------+---------+--------------+------------------+------------------+--------------+-------------------+\n\n</div>"]}}],"execution_count":35},{"cell_type":"markdown","source":["d\n * Here we see that the count for each of the columns now is 43400. This indicates the missing values in \"smoking_status\" and \"bmi\" has been handled"],"metadata":{}},{"cell_type":"markdown","source":["### Data Modelling"],"metadata":{}},{"cell_type":"markdown","source":["* Now, Lets work on categorical columns.\n* Perfroming \"StringIndexer -> OneHotEncoder -> VectorAssembler\""],"metadata":{}},{"cell_type":"code","source":["# indexing all categorical columns in the dataset\n\nindexer1 = StringIndexer(inputCol=\"gender\", outputCol=\"genderIndex\")\nindexer2 = StringIndexer(inputCol=\"ever_married\", outputCol=\"ever_marriedIndex\")\nindexer3 = StringIndexer(inputCol=\"work_type\", outputCol=\"work_typeIndex\")\nindexer4 = StringIndexer(inputCol=\"Residence_type\", outputCol=\"Residence_typeIndex\")\nindexer5 = StringIndexer(inputCol=\"smoking_status\", outputCol=\"smoking_statusIndex\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"code","source":["# Doing one hot encoding of indexed data\n\nfrom pyspark.ml.feature import OneHotEncoderEstimator\nencoder = OneHotEncoderEstimator(inputCols=[\"genderIndex\",\"ever_marriedIndex\",\"work_typeIndex\",\"Residence_typeIndex\",\"smoking_statusIndex\"],\n                                 outputCols=[\"genderVec\",\"ever_marriedVec\",\"work_typeVec\",\"Residence_typeVec\",\"smoking_statusVec\"])"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"code","source":["#The next step is to create an assembler, that combines a given list of columns into a single vector column to train ML model. We will use the vector columns, that we got after one_hot_encoding.\nfrom pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(inputCols=[\n 'genderVec',\n 'age',\n 'hypertension',\n 'heart_disease',\n 'ever_marriedVec',\n 'work_typeVec',\n 'Residence_typeVec',\n 'avg_glucose_level',\n 'bmi',\n 'smoking_statusVec'] , outputCol='features')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"markdown","source":["* We have complex task that contains bunch of stages, these bunch of satges needs to be performed to process data. To wrap all of that Spark ML represents such a workflow as a Pipeline, which consists of a sequence of PipelineStages to be run in a specific order."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n# Pipeline basic to be shared across model fitting and testing\npipeline = Pipeline(stages=[]) # Must initialize with empty list!"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":43},{"cell_type":"code","source":["basePipeline = [indexer1, indexer2, indexer3, indexer4, indexer5, encoder, assembler]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":44},{"cell_type":"code","source":["from pyspark.ml.classification import RandomForestClassifier, GBTClassifier, NaiveBayes, LogisticRegression, DecisionTreeClassifier\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n\ndtc = DecisionTreeClassifier(labelCol='stroke',featuresCol='features')\npl_dtc=basePipeline+[dtc]\npg_dtc = ParamGridBuilder()\\\n        .baseOn({pipeline.stages: pl_dtc})\\\n        .build()\n\nrf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"stroke\",numTrees=25)\npl_rf = basePipeline + [rf]\npg_rf = ParamGridBuilder()\\\n      .baseOn({pipeline.stages: pl_rf})\\\n      .build()\n\nnb = NaiveBayes(labelCol='stroke',featuresCol='features')\npl_nb = basePipeline + [nb]\npg_nb = ParamGridBuilder()\\\n.baseOn({pipeline.stages: pl_nb})\\\n.addGrid(nb.smoothing,[0.4,1.0])\\\n.build()\n\nparamGrid = pg_dtc + pg_rf + pg_nb\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":45},{"cell_type":"markdown","source":["###Data Transformation"],"metadata":{}},{"cell_type":"code","source":["splitted_data = train_f.randomSplit([0.7,0.3], 24)\ntrain_data = splitted_data[0]\ntest_data = splitted_data[1]\nprint(\"Number of training records: \" + str(train_data.count()))\nprint(\"Number of testing records : \" + str(test_data.count()))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Number of training records: 30379\nNumber of testing records : 13021\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["cv = CrossValidator()\\\n    .setEstimator(pipeline)\\\n    .setEvaluator(MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\"))\\\n    .setEstimatorParamMaps(paramGrid)\\\n    .setNumFolds(3) "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":48},{"cell_type":"code","source":["fittedModel1=cv.fit(train_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/spark/python/pyspark/ml/util.py:791: UserWarning: Can not find mlflow. To enable mlflow logging, install MLflow library from PyPi.\n  warnings.warn(_MLflowInstrumentation._NO_MLFLOW_WARNING)\n</div>"]}}],"execution_count":49},{"cell_type":"code","source":["train_data.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+------------------+---------------+------+\n id|gender| age|hypertension|heart_disease|ever_married|    work_type|Residence_type|avg_glucose_level|               bmi| smoking_status|stroke|\n+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+------------------+---------------+------+\n  6|Female|21.0|           0|            0|          No|      Private|         Urban|            75.73|28.605038390004545|   never smoked|     0|\n  8|Female|79.0|           0|            0|         Yes|      Private|         Urban|            99.23|              25.1|        No Info|     0|\n 11|Female|54.0|           0|            0|         Yes|Self-employed|         Urban|            83.01|28.605038390004545|   never smoked|     0|\n 12|Female|56.0|           0|            0|         Yes|      Private|         Rural|            102.3|              54.9|formerly smoked|     0|\n 14|Female|67.0|           0|            0|         Yes|      Private|         Urban|           111.04|              25.1|   never smoked|     0|\n 15|Female|74.0|           0|            0|          No|     Govt_job|         Rural|           104.73|28.605038390004545|        No Info|     0|\n 22|  Male| 5.0|           0|            0|          No|     children|         Rural|           100.81|              20.6|        No Info|     0|\n 25|Female| 4.0|           0|            0|          No|     children|         Urban|             71.9|              14.1|        No Info|     0|\n 36|  Male| 5.0|           0|            0|          No|     children|         Rural|            91.85|              16.6|        No Info|     0|\n 37|Female|13.0|           0|            0|          No|      Private|         Rural|            83.99|              22.5|        No Info|     0|\n 39|  Male|52.0|           1|            0|         Yes|Self-employed|         Rural|           200.37|28.605038390004545|         smokes|     0|\n 40|Female|36.0|           0|            0|          No|      Private|         Rural|            65.85|              59.0|   never smoked|     0|\n 41|  Male|74.0|           0|            0|         Yes|     Govt_job|         Rural|           201.58|              25.1|formerly smoked|     0|\n 46|Female|46.0|           0|            0|         Yes|     Govt_job|         Urban|            80.97|              25.4|   never smoked|     0|\n 47|Female|28.0|           0|            0|         Yes|      Private|         Urban|            94.49|              20.0|   never smoked|     0|\n 48|Female|58.0|           0|            0|         Yes|     Govt_job|         Rural|            77.54|              18.1|   never smoked|     0|\n 51|  Male|59.0|           0|            0|         Yes|     Govt_job|         Urban|            91.02|28.605038390004545|   never smoked|     0|\n 54|Female|52.0|           0|            0|         Yes|Self-employed|         Urban|           199.55|28.605038390004545|formerly smoked|     0|\n 56|  Male|48.0|           0|            0|         Yes|Self-employed|         Rural|           201.55|              39.7|formerly smoked|     0|\n 65|Female|26.0|           0|            0|         Yes|      Private|         Rural|           119.28|              32.4|formerly smoked|     0|\n+---+------+----+------------+-------------+------------+-------------+--------------+-----------------+------------------+---------------+------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":50},{"cell_type":"markdown","source":["###The Best Model"],"metadata":{}},{"cell_type":"code","source":["import numpy as np\n# BinaryClassificationEvaluator defaults to ROC AUC, so higher is better\n# http://gim.unmc.edu/dxtests/roc3.htm\nfittedModel1.getEstimatorParamMaps()[ np.argmax(fittedModel1.avgMetrics) ]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[28]: {Param(parent=&#39;Pipeline_699e189f94b6&#39;, name=&#39;stages&#39;, doc=&#39;a list of pipeline stages&#39;): [StringIndexer_2f2fdfff9653,\n  StringIndexer_e979ef558108,\n  StringIndexer_65ef13cccb45,\n  StringIndexer_7d85213125a3,\n  StringIndexer_50a2ee2c5949,\n  OneHotEncoderEstimator_1139ed206783,\n  VectorAssembler_06016f2f5c8d,\n  RandomForestClassifier_a7c0f599b9c6]}</div>"]}}],"execution_count":52},{"cell_type":"markdown","source":["* As is clearly evident, RandomeForestCLassifier is coming out to be the best model"],"metadata":{}},{"cell_type":"markdown","source":["###The Worst Model"],"metadata":{}},{"cell_type":"code","source":["fittedModel1.getEstimatorParamMaps()[ np.argmin(fittedModel1.avgMetrics) ]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[29]: {Param(parent=&#39;Pipeline_699e189f94b6&#39;, name=&#39;stages&#39;, doc=&#39;a list of pipeline stages&#39;): [StringIndexer_2f2fdfff9653,\n  StringIndexer_e979ef558108,\n  StringIndexer_65ef13cccb45,\n  StringIndexer_7d85213125a3,\n  StringIndexer_50a2ee2c5949,\n  OneHotEncoderEstimator_1139ed206783,\n  VectorAssembler_06016f2f5c8d,\n  NaiveBayes_70c961cbfbc1],\n Param(parent=&#39;NaiveBayes_70c961cbfbc1&#39;, name=&#39;smoothing&#39;, doc=&#39;The smoothing parameter, should be &gt;= 0, default is 1.0&#39;): 1.0}</div>"]}}],"execution_count":55},{"cell_type":"markdown","source":["* NaiveBayes is worst model"],"metadata":{}},{"cell_type":"code","source":["import re\ndef paramGrid_model_name(model):\n  params = [v for v in model.values() if type(v) is not list]\n  name = [v[-1] for v in model.values() if type(v) is list][0]\n  name = re.match(r'([a-zA-Z]*)', str(name)).groups()[0]\n  return \"{}{}\".format(name,params)\n\n# Resulting metric and model description\n# get the measure from the CrossValidator, cvModel.avgMetrics\n# get the model name & params from the paramGrid\n# put them together here:\nkmeans_measures = zip(fittedModel1.avgMetrics, [paramGrid_model_name(m) for m in paramGrid])\nmetrics,model_names = zip(*kmeans_measures)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":57},{"cell_type":"code","source":["import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.clf() # clear figure\nfig = plt.figure( figsize=(5, 5))\nplt.style.use('fivethirtyeight')\naxis = fig.add_axes([0.1, 0.3, 0.8, 0.6])\n# plot the metrics as Y\n#plt.plot(range(len(model_names)),metrics)\nplt.bar(range(len(model_names)),metrics)\n# plot the model name & param as X labels\nplt.xticks(range(len(model_names)), model_names, rotation=70, fontsize=6)\nplt.yticks(fontsize=6)\n#plt.xlabel('model',fontsize=8)\nplt.ylabel('ROC AUC (better is greater)',fontsize=8)\nplt.title('Model evaluations')\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["### Predictions"],"metadata":{}},{"cell_type":"code","source":["predictions=fittedModel1.transform(test_data)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":60},{"cell_type":"code","source":["## Make predictions on test documents. \n# CrossValidator.fit() is in cvModel, which is the best model found (rfModel).\npredictions.select(\"prediction\", \"stroke\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------+\nprediction|stroke|\n+----------+------+\n       0.0|     0|\n       0.0|     0|\n       0.0|     0|\n       0.0|     0|\n       0.0|     0|\n+----------+------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":61},{"cell_type":"markdown","source":["### Model Evaluation"],"metadata":{}},{"cell_type":"code","source":["# Select (prediction, true label) and compute test error\n\nacc_evaluator = MulticlassClassificationEvaluator(labelCol=\"stroke\", predictionCol=\"prediction\", metricName=\"accuracy\")\nrfc_acc = acc_evaluator.evaluate(predictions)\nprint('A Random Forest Classifier had an accuracy of: {0:2.2f}%'.format(rfc_acc*100))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">A Random Forest Classifier had an accuracy of: 98.30%\n</div>"]}}],"execution_count":63},{"cell_type":"markdown","source":["#### So, the best model which is the RandomForestClassifier gives us the accuracy of 98.30% which is very high, so our model's accuracy seems to be great"],"metadata":{}},{"cell_type":"markdown","source":["####Lets view result of the best model"],"metadata":{}},{"cell_type":"code","source":["correct = predictions.where(\"(stroke = prediction)\").count()\nincorrect = predictions.where(\"(stroke != prediction)\").count()\n\nresultDF = sqlContext.createDataFrame([['correct', correct], ['incorrect', incorrect]], ['metric', 'value'])\ndisplay(resultDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>metric</th><th>value</th></tr></thead><tbody><tr><td>correct</td><td>12800</td></tr><tr><td>incorrect</td><td>221</td></tr></tbody></table></div>"]}}],"execution_count":66},{"cell_type":"markdown","source":["## Result Visualization"],"metadata":{}},{"cell_type":"code","source":["correct = predictions.where(\"(stroke = prediction)\").count()\nincorrect = predictions.where(\"(stroke != prediction)\").count()\n\nresultDF = sqlContext.createDataFrame([['correct', correct], ['incorrect', incorrect]], ['metric', 'value'])\ndisplay(resultDF)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>metric</th><th>value</th></tr></thead><tbody><tr><td>correct</td><td>12800</td></tr><tr><td>incorrect</td><td>221</td></tr></tbody></table></div>"]}}],"execution_count":68},{"cell_type":"markdown","source":["#### As is evident from the above visualization, our model predicted very high correct classifications over incorrect classifications which clearly indicates model is performing good."],"metadata":{}},{"cell_type":"markdown","source":["####Confusion Matrix"],"metadata":{}},{"cell_type":"code","source":["counts = [predictions.where('stroke=1').count(), predictions.where('prediction=1').count(),\n          predictions.where('stroke=0').count(), predictions.where('prediction=0').count()]\nnames = ['actual 1', 'predicted 1', 'actual 0', 'predicted 0']\ndisplay(sqlContext.createDataFrame(zip(names,counts),['Measure','Value']))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Measure</th><th>Value</th></tr></thead><tbody><tr><td>actual 1</td><td>221</td></tr><tr><td>predicted 1</td><td>0</td></tr><tr><td>actual 0</td><td>12800</td></tr><tr><td>predicted 0</td><td>13021</td></tr></tbody></table></div>"]}}],"execution_count":71},{"cell_type":"code","source":["counts = [predictions.where('stroke=1').count(), predictions.where('prediction=1').count(),\n          predictions.where('stroke=0').count(), predictions.where('prediction=0').count()]\nnames = ['actual 1', 'predicted 1', 'actual 0', 'predicted 0']\ndisplay(sqlContext.createDataFrame(zip(names,counts),['Measure','Value']))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Measure</th><th>Value</th></tr></thead><tbody><tr><td>actual 1</td><td>221</td></tr><tr><td>predicted 1</td><td>0</td></tr><tr><td>actual 0</td><td>12800</td></tr><tr><td>predicted 0</td><td>13021</td></tr></tbody></table></div>"]}}],"execution_count":72},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":73}],"metadata":{"name":"Project","notebookId":2718238233647172},"nbformat":4,"nbformat_minor":0}
